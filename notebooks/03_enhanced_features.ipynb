{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Phase 2: Enhanced Feature Engineering\n",
    "\n",
    "**Objective:** Improve model performance by adding:\n",
    "1. **Glassnode on-chain metrics** - Regime/sentiment indicators\n",
    "2. **Technical indicators** - RSI, MACD, Bollinger Bands\n",
    "\n",
    "## Key Hypothesis\n",
    "The challenge mentions \"latent regime dynamics\". On-chain metrics like MVRV, Fear & Greed, \n",
    "and SOPR may help identify these regimes and improve our predictions.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standard imports\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from pathlib import Path\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# ML imports\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, roc_auc_score\n",
    "\n",
    "# Set style\n",
    "plt.style.use('seaborn-v0_8-whitegrid')\n",
    "plt.rcParams['figure.figsize'] = (14, 6)\n",
    "\n",
    "# Add src to path\n",
    "import sys\n",
    "sys.path.insert(0, str(Path.cwd().parent / 'src'))\n",
    "\n",
    "from data import load_all_data\n",
    "from features import compute_rolling_returns, compute_rolling_volatility, compute_rsi, compute_bollinger_bands\n",
    "from labels import create_cost_adjusted_labels, analyze_label_distribution\n",
    "from metrics import compute_all_metrics\n",
    "from backtesting import compute_strategy_returns, compute_portfolio_returns, compute_equity_curve"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load all data\n",
    "trade_log, prices, glassnode = load_all_data()\n",
    "\n",
    "# Align trade_log and prices\n",
    "common_idx = trade_log.index.intersection(prices.index)\n",
    "common_assets = trade_log.columns.intersection(prices.columns)\n",
    "\n",
    "signals = trade_log.loc[common_idx, common_assets]\n",
    "prices_aligned = prices.loc[common_idx, common_assets]\n",
    "\n",
    "print(f\"Signals: {signals.shape}\")\n",
    "print(f\"Prices: {prices_aligned.shape}\")\n",
    "print(f\"Glassnode: {glassnode.shape}\")\n",
    "print(f\"\\nPeriod: {signals.index.min()} to {signals.index.max()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 2. Glassnode Feature Selection\n",
    "\n",
    "### Key On-Chain Metrics\n",
    "\n",
    "| Feature | What it Measures | Interpretation |\n",
    "|---------|------------------|----------------|\n",
    "| MVRV Z-Score | Market vs Realized Value | >3 = overvalued (top), <0 = undervalued (bottom) |\n",
    "| Fear & Greed | Market Sentiment | 0-25 = fear (buy signal), 75-100 = greed (sell signal) |\n",
    "| SOPR | Profit/Loss Ratio | >1 = profit taking, <1 = capitulation |\n",
    "| Puell Multiple | Miner Revenue | >4 = miners selling (bearish), <0.5 = accumulation |\n",
    "| % Supply in Profit | How much BTC is profitable | Low = capitulation, High = euphoria |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select key Glassnode features\n",
    "GLASSNODE_FEATURES = [\n",
    "    # Valuation\n",
    "    'btc_mvrv_z_score',\n",
    "    'btc_puell_multiple',\n",
    "    'reserve_risk',\n",
    "    \n",
    "    # Sentiment\n",
    "    'btc_fear_greed_index',\n",
    "    'btc_adjusted_sopr',\n",
    "    \n",
    "    # On-chain activity\n",
    "    'btc_percent_upply_in_profit',\n",
    "    'btc_network_value_to_transactions_signal',\n",
    "    \n",
    "    # Derivatives\n",
    "    'btc_futures_perpetual_funding_rate_mean',\n",
    "    \n",
    "    # Supply dynamics\n",
    "    'vocdd',\n",
    "    'mvocdd',\n",
    "]\n",
    "\n",
    "# Check availability\n",
    "available_gn = [f for f in GLASSNODE_FEATURES if f in glassnode.columns]\n",
    "print(f\"Available Glassnode features: {len(available_gn)}/{len(GLASSNODE_FEATURES)}\")\n",
    "for f in available_gn:\n",
    "    print(f\"  - {f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare Glassnode features\n",
    "# Note: Glassnode is daily, we need to align with our 3-hour data\n",
    "\n",
    "gn_selected = glassnode[available_gn].copy()\n",
    "\n",
    "# Make timezone-naive for alignment\n",
    "signals_naive = signals.copy()\n",
    "signals_naive.index = signals_naive.index.tz_localize(None)\n",
    "\n",
    "# Resample Glassnode to match signals (forward fill daily to 3-hourly)\n",
    "# First, get the date part of each signal timestamp\n",
    "gn_aligned = pd.DataFrame(index=signals_naive.index)\n",
    "\n",
    "for col in available_gn:\n",
    "    # Create a series indexed by date\n",
    "    gn_series = gn_selected[col]\n",
    "    \n",
    "    # For each timestamp in signals, get the Glassnode value for that date\n",
    "    aligned_values = []\n",
    "    for ts in signals_naive.index:\n",
    "        date = ts.normalize()  # Get just the date part\n",
    "        if date in gn_series.index:\n",
    "            aligned_values.append(gn_series.loc[date])\n",
    "        else:\n",
    "            # Find the most recent available date\n",
    "            available_dates = gn_series.index[gn_series.index <= date]\n",
    "            if len(available_dates) > 0:\n",
    "                aligned_values.append(gn_series.loc[available_dates[-1]])\n",
    "            else:\n",
    "                aligned_values.append(np.nan)\n",
    "    \n",
    "    gn_aligned[col] = aligned_values\n",
    "\n",
    "print(f\"Aligned Glassnode features: {gn_aligned.shape}\")\n",
    "print(f\"Missing values: {gn_aligned.isna().sum().sum()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize key Glassnode features\n",
    "fig, axes = plt.subplots(3, 2, figsize=(14, 10))\n",
    "\n",
    "features_to_plot = [\n",
    "    ('btc_mvrv_z_score', 'MVRV Z-Score'),\n",
    "    ('btc_fear_greed_index', 'Fear & Greed Index'),\n",
    "    ('btc_adjusted_sopr', 'Adjusted SOPR'),\n",
    "    ('btc_puell_multiple', 'Puell Multiple'),\n",
    "    ('btc_percent_upply_in_profit', '% Supply in Profit'),\n",
    "    ('btc_futures_perpetual_funding_rate_mean', 'Funding Rate Mean'),\n",
    "]\n",
    "\n",
    "for ax, (col, title) in zip(axes.flat, features_to_plot):\n",
    "    if col in gn_aligned.columns:\n",
    "        ax.plot(gn_aligned.index, gn_aligned[col], alpha=0.7)\n",
    "        ax.set_title(title)\n",
    "        ax.tick_params(axis='x', rotation=45)\n",
    "        ax.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 3. Technical Indicators"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute technical indicators for each asset\n",
    "def compute_macd(prices, fast=96, slow=208, signal=72):\n",
    "    \"\"\"\n",
    "    Compute MACD (Moving Average Convergence Divergence).\n",
    "    Default periods adjusted for 3-hour data:\n",
    "    - Fast: 12 days = 96 periods\n",
    "    - Slow: 26 days = 208 periods  \n",
    "    - Signal: 9 days = 72 periods\n",
    "    \"\"\"\n",
    "    features = pd.DataFrame(index=prices.index)\n",
    "    \n",
    "    for col in prices.columns:\n",
    "        ema_fast = prices[col].ewm(span=fast, adjust=False).mean()\n",
    "        ema_slow = prices[col].ewm(span=slow, adjust=False).mean()\n",
    "        \n",
    "        macd_line = ema_fast - ema_slow\n",
    "        signal_line = macd_line.ewm(span=signal, adjust=False).mean()\n",
    "        histogram = macd_line - signal_line\n",
    "        \n",
    "        # Normalize by price to make comparable across assets\n",
    "        features[f'{col}_macd'] = macd_line / prices[col]\n",
    "        features[f'{col}_macd_signal'] = signal_line / prices[col]\n",
    "        features[f'{col}_macd_hist'] = histogram / prices[col]\n",
    "    \n",
    "    return features\n",
    "\n",
    "# Compute all technical indicators\n",
    "print(\"Computing technical indicators...\")\n",
    "\n",
    "# Price-based features (from Phase 1)\n",
    "return_features = compute_rolling_returns(prices_aligned, windows=[1, 8, 56, 224])\n",
    "vol_features = compute_rolling_volatility(prices_aligned, windows=[56, 224])\n",
    "\n",
    "# New technical indicators\n",
    "rsi_features = compute_rsi(prices_aligned, window=112)  # 14 days\n",
    "bb_features = compute_bollinger_bands(prices_aligned, window=160)  # 20 days\n",
    "macd_features = compute_macd(prices_aligned)\n",
    "\n",
    "print(f\"Return features: {return_features.shape}\")\n",
    "print(f\"Volatility features: {vol_features.shape}\")\n",
    "print(f\"RSI features: {rsi_features.shape}\")\n",
    "print(f\"Bollinger Band features: {bb_features.shape}\")\n",
    "print(f\"MACD features: {macd_features.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 4. Combine All Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combine all price-based features\n",
    "price_features = pd.concat([\n",
    "    return_features,\n",
    "    vol_features,\n",
    "    rsi_features,\n",
    "    bb_features,\n",
    "    macd_features\n",
    "], axis=1)\n",
    "\n",
    "print(f\"Combined price features: {price_features.shape}\")\n",
    "print(f\"\\nFeature types:\")\n",
    "print(f\"  - Returns: {len([c for c in price_features.columns if 'return' in c])}\")\n",
    "print(f\"  - Volatility: {len([c for c in price_features.columns if 'volatility' in c])}\")\n",
    "print(f\"  - RSI: {len([c for c in price_features.columns if 'rsi' in c])}\")\n",
    "print(f\"  - Bollinger: {len([c for c in price_features.columns if 'bb' in c])}\")\n",
    "print(f\"  - MACD: {len([c for c in price_features.columns if 'macd' in c])}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 5. Create Labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create labels (same as Phase 1)\n",
    "HORIZON = 8  # 1 day\n",
    "\n",
    "labels = create_cost_adjusted_labels(\n",
    "    prices_aligned,\n",
    "    signals,\n",
    "    horizon=HORIZON,\n",
    "    entry_cost=0.001,\n",
    "    exit_cost=0.001\n",
    ")\n",
    "\n",
    "label_stats = analyze_label_distribution(labels, signals)\n",
    "print(f\"Labels: {labels.shape}\")\n",
    "print(f\"Positive rate: {label_stats['overall']['positive_rate']*100:.1f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 6. Prepare Stacked Data with All Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_enhanced_data(price_features, glassnode_features, labels, signals):\n",
    "    \"\"\"\n",
    "    Prepare stacked data with both price-based and Glassnode features.\n",
    "    \n",
    "    Price features are asset-specific (renamed to generic).\n",
    "    Glassnode features are global (same for all assets).\n",
    "    \"\"\"\n",
    "    data_rows = []\n",
    "    \n",
    "    # Make signals index timezone-naive for comparison\n",
    "    signals_naive = signals.copy()\n",
    "    signals_naive.index = signals_naive.index.tz_localize(None)\n",
    "    \n",
    "    labels_naive = labels.copy()\n",
    "    labels_naive.index = labels_naive.index.tz_localize(None)\n",
    "    \n",
    "    price_features_naive = price_features.copy()\n",
    "    price_features_naive.index = price_features_naive.index.tz_localize(None)\n",
    "    \n",
    "    for timestamp in labels_naive.index:\n",
    "        if timestamp not in price_features_naive.index:\n",
    "            continue\n",
    "        if timestamp not in glassnode_features.index:\n",
    "            continue\n",
    "            \n",
    "        for asset in labels_naive.columns:\n",
    "            # Only include rows where signal = 1\n",
    "            if signals_naive.loc[timestamp, asset] != 1:\n",
    "                continue\n",
    "                \n",
    "            label_val = labels_naive.loc[timestamp, asset]\n",
    "            if pd.isna(label_val):\n",
    "                continue\n",
    "            \n",
    "            # Get asset-specific price features\n",
    "            asset_cols = [c for c in price_features_naive.columns if c.startswith(asset + '_')]\n",
    "            if not asset_cols:\n",
    "                continue\n",
    "                \n",
    "            price_row = price_features_naive.loc[timestamp, asset_cols]\n",
    "            if price_row.isna().any():\n",
    "                continue\n",
    "            \n",
    "            # Get Glassnode features (global)\n",
    "            gn_row = glassnode_features.loc[timestamp]\n",
    "            if gn_row.isna().any():\n",
    "                continue\n",
    "            \n",
    "            # Rename price features to be asset-agnostic\n",
    "            renamed_price = {}\n",
    "            for col in asset_cols:\n",
    "                new_name = col.replace(asset + '_', '')\n",
    "                renamed_price[new_name] = price_row[col]\n",
    "            \n",
    "            # Combine all features\n",
    "            row_data = {\n",
    "                'timestamp': timestamp,\n",
    "                'asset': asset,\n",
    "                'label': label_val,\n",
    "                **renamed_price,\n",
    "                **gn_row.to_dict()\n",
    "            }\n",
    "            data_rows.append(row_data)\n",
    "    \n",
    "    df = pd.DataFrame(data_rows)\n",
    "    df = df.set_index(['timestamp', 'asset'])\n",
    "    \n",
    "    y = df['label']\n",
    "    X = df.drop('label', axis=1)\n",
    "    \n",
    "    return X, y\n",
    "\n",
    "X, y = prepare_enhanced_data(price_features, gn_aligned, labels, signals)\n",
    "print(f\"Enhanced data: X = {X.shape}, y = {y.shape}\")\n",
    "print(f\"\\nFeature columns ({len(X.columns)}):\")\n",
    "for i, col in enumerate(X.columns):\n",
    "    print(f\"  {i+1:2d}. {col}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check for any remaining NaN\n",
    "print(f\"NaN in X: {X.isna().sum().sum()}\")\n",
    "print(f\"NaN in y: {y.isna().sum()}\")\n",
    "\n",
    "if X.isna().sum().sum() > 0:\n",
    "    print(\"\\nColumns with NaN:\")\n",
    "    nan_cols = X.isna().sum()\n",
    "    print(nan_cols[nan_cols > 0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 7. Walk-Forward Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def walk_forward_cv(X, y, train_size, test_size, step_size=None):\n",
    "    \"\"\"Generate walk-forward cross-validation splits.\"\"\"\n",
    "    if step_size is None:\n",
    "        step_size = test_size\n",
    "    \n",
    "    timestamps = X.index.get_level_values('timestamp').unique().sort_values()\n",
    "    n_timestamps = len(timestamps)\n",
    "    \n",
    "    fold = 0\n",
    "    start = 0\n",
    "    \n",
    "    while start + train_size + test_size <= n_timestamps:\n",
    "        train_end = start + train_size\n",
    "        test_end = train_end + test_size\n",
    "        \n",
    "        train_timestamps = timestamps[start:train_end]\n",
    "        test_timestamps = timestamps[train_end:test_end]\n",
    "        \n",
    "        train_idx = X.index.get_level_values('timestamp').isin(train_timestamps)\n",
    "        test_idx = X.index.get_level_values('timestamp').isin(test_timestamps)\n",
    "        \n",
    "        fold_info = {\n",
    "            'fold': fold,\n",
    "            'train_start': train_timestamps[0],\n",
    "            'train_end': train_timestamps[-1],\n",
    "            'test_start': test_timestamps[0],\n",
    "            'test_end': test_timestamps[-1],\n",
    "        }\n",
    "        \n",
    "        yield train_idx, test_idx, fold_info\n",
    "        \n",
    "        start += step_size\n",
    "        fold += 1\n",
    "\n",
    "# CV parameters\n",
    "n_timestamps = X.index.get_level_values('timestamp').nunique()\n",
    "TRAIN_SIZE = int(n_timestamps * 0.6)\n",
    "TEST_SIZE = int(n_timestamps * 0.1)\n",
    "STEP_SIZE = TEST_SIZE\n",
    "\n",
    "print(f\"Total timestamps: {n_timestamps}\")\n",
    "print(f\"Train: {TRAIN_SIZE}, Test: {TEST_SIZE}, Step: {STEP_SIZE}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train with Logistic Regression (same as Phase 1 for fair comparison)\n",
    "def train_model(X_train, y_train):\n",
    "    scaler = StandardScaler()\n",
    "    X_scaled = scaler.fit_transform(X_train)\n",
    "    \n",
    "    model = LogisticRegression(\n",
    "        C=1.0,\n",
    "        class_weight='balanced',\n",
    "        max_iter=1000,\n",
    "        random_state=42\n",
    "    )\n",
    "    model.fit(X_scaled, y_train)\n",
    "    \n",
    "    return model, scaler\n",
    "\n",
    "# Run walk-forward validation\n",
    "all_predictions = []\n",
    "fold_results = []\n",
    "\n",
    "for train_idx, test_idx, fold_info in walk_forward_cv(X, y, TRAIN_SIZE, TEST_SIZE, STEP_SIZE):\n",
    "    X_train, X_test = X[train_idx], X[test_idx]\n",
    "    y_train, y_test = y[train_idx], y[test_idx]\n",
    "    \n",
    "    model, scaler = train_model(X_train, y_train)\n",
    "    \n",
    "    X_test_scaled = scaler.transform(X_test)\n",
    "    y_prob = model.predict_proba(X_test_scaled)[:, 1]\n",
    "    y_pred = (y_prob > 0.5).astype(int)\n",
    "    \n",
    "    pred_df = pd.DataFrame({\n",
    "        'probability': y_prob,\n",
    "        'prediction': y_pred,\n",
    "        'actual': y_test.values\n",
    "    }, index=y_test.index)\n",
    "    all_predictions.append(pred_df)\n",
    "    \n",
    "    fold_info['accuracy'] = accuracy_score(y_test, y_pred)\n",
    "    fold_info['auc'] = roc_auc_score(y_test, y_prob) if len(np.unique(y_test)) > 1 else 0.5\n",
    "    fold_results.append(fold_info)\n",
    "    \n",
    "    print(f\"Fold {fold_info['fold']}: Acc={fold_info['accuracy']:.3f}, AUC={fold_info['auc']:.3f}\")\n",
    "\n",
    "predictions_df = pd.concat(all_predictions)\n",
    "\n",
    "# Summary\n",
    "results_df = pd.DataFrame(fold_results)\n",
    "print(f\"\\nMean Accuracy: {results_df['accuracy'].mean():.3f} (+/- {results_df['accuracy'].std():.3f})\")\n",
    "print(f\"Mean AUC: {results_df['auc'].mean():.3f} (+/- {results_df['auc'].std():.3f})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 8. Evaluate Strategy Performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_filtered_signals(predictions_df, signals, threshold=0.5):\n",
    "    \"\"\"Apply ML filter to baseline signals.\"\"\"\n",
    "    # Make signals timezone-naive\n",
    "    signals_naive = signals.copy()\n",
    "    signals_naive.index = signals_naive.index.tz_localize(None)\n",
    "    \n",
    "    pred_timestamps = predictions_df.index.get_level_values('timestamp').unique()\n",
    "    filtered = signals_naive.loc[pred_timestamps].copy()\n",
    "    \n",
    "    for (timestamp, asset), row in predictions_df.iterrows():\n",
    "        if asset in filtered.columns and row['probability'] <= threshold:\n",
    "            filtered.loc[timestamp, asset] = 0\n",
    "    \n",
    "    return filtered\n",
    "\n",
    "# Get test period data\n",
    "pred_timestamps = predictions_df.index.get_level_values('timestamp').unique()\n",
    "\n",
    "# Make prices timezone-naive\n",
    "prices_naive = prices_aligned.copy()\n",
    "prices_naive.index = prices_naive.index.tz_localize(None)\n",
    "\n",
    "signals_naive = signals.copy()\n",
    "signals_naive.index = signals_naive.index.tz_localize(None)\n",
    "\n",
    "baseline_signals = signals_naive.loc[pred_timestamps]\n",
    "comparison_prices = prices_naive.loc[pred_timestamps]\n",
    "\n",
    "# Baseline performance\n",
    "baseline_returns = compute_strategy_returns(baseline_signals, comparison_prices, transaction_cost=0.001)\n",
    "baseline_portfolio = compute_portfolio_returns(baseline_returns)\n",
    "baseline_metrics = compute_all_metrics(baseline_portfolio.dropna())\n",
    "\n",
    "print(\"BASELINE (Test Period):\")\n",
    "print(f\"  Sharpe Ratio: {baseline_metrics['sharpe_ratio']:.3f}\")\n",
    "print(f\"  Total Return: {baseline_metrics['total_return']*100:.2f}%\")\n",
    "print(f\"  Max Drawdown: {baseline_metrics['max_drawdown']*100:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test different thresholds\n",
    "thresholds = [0.3, 0.4, 0.5, 0.6, 0.7]\n",
    "threshold_results = []\n",
    "\n",
    "print(\"\\nML-FILTERED STRATEGY (Enhanced Features):\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "for threshold in thresholds:\n",
    "    filtered_signals = create_filtered_signals(predictions_df, signals, threshold)\n",
    "    filtered_returns = compute_strategy_returns(filtered_signals, comparison_prices, transaction_cost=0.001)\n",
    "    filtered_portfolio = compute_portfolio_returns(filtered_returns)\n",
    "    filtered_metrics = compute_all_metrics(filtered_portfolio.dropna())\n",
    "    \n",
    "    n_trades_baseline = baseline_signals.diff().abs().sum().sum() / 2\n",
    "    n_trades_filtered = filtered_signals.diff().abs().sum().sum() / 2\n",
    "    \n",
    "    result = {\n",
    "        'threshold': threshold,\n",
    "        'sharpe': filtered_metrics['sharpe_ratio'],\n",
    "        'total_return': filtered_metrics['total_return'],\n",
    "        'max_drawdown': filtered_metrics['max_drawdown'],\n",
    "        'trade_reduction': (n_trades_baseline - n_trades_filtered) / n_trades_baseline * 100\n",
    "    }\n",
    "    threshold_results.append(result)\n",
    "    \n",
    "    sharpe_change = (result['sharpe'] - baseline_metrics['sharpe_ratio']) / baseline_metrics['sharpe_ratio'] * 100\n",
    "    print(f\"\\nThreshold τ = {threshold}:\")\n",
    "    print(f\"  Sharpe: {result['sharpe']:.3f} ({sharpe_change:+.1f}% vs baseline)\")\n",
    "    print(f\"  Return: {result['total_return']*100:.2f}%\")\n",
    "    print(f\"  Max DD: {result['max_drawdown']*100:.2f}%\")\n",
    "    print(f\"  Trade Reduction: {result['trade_reduction']:.1f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot comparison\n",
    "threshold_df = pd.DataFrame(threshold_results)\n",
    "\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# Sharpe ratio\n",
    "axes[0].plot(threshold_df['threshold'], threshold_df['sharpe'], 'go-', markersize=10, linewidth=2, label='Phase 2 (Enhanced)')\n",
    "axes[0].axhline(y=baseline_metrics['sharpe_ratio'], color='r', linestyle='--', linewidth=2, label='Baseline')\n",
    "axes[0].set_xlabel('Threshold (τ)', fontsize=12)\n",
    "axes[0].set_ylabel('Sharpe Ratio', fontsize=12)\n",
    "axes[0].set_title('Sharpe Ratio vs Threshold', fontsize=14)\n",
    "axes[0].legend(fontsize=11)\n",
    "axes[0].grid(True, alpha=0.3)\n",
    "\n",
    "# Total return vs trade reduction\n",
    "axes[1].scatter(threshold_df['trade_reduction'], threshold_df['total_return']*100, \n",
    "                c=threshold_df['threshold'], cmap='viridis', s=200, edgecolors='black')\n",
    "axes[1].axhline(y=baseline_metrics['total_return']*100, color='r', linestyle='--', label='Baseline Return')\n",
    "axes[1].set_xlabel('Trade Reduction (%)', fontsize=12)\n",
    "axes[1].set_ylabel('Total Return (%)', fontsize=12)\n",
    "axes[1].set_title('Return vs Trade Reduction', fontsize=14)\n",
    "axes[1].legend()\n",
    "axes[1].grid(True, alpha=0.3)\n",
    "\n",
    "# Add colorbar\n",
    "cbar = plt.colorbar(axes[1].collections[0], ax=axes[1])\n",
    "cbar.set_label('Threshold τ')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 9. Feature Importance Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train on all data to get feature importance\n",
    "model_final, scaler_final = train_model(X, y)\n",
    "\n",
    "importance = pd.DataFrame({\n",
    "    'feature': X.columns,\n",
    "    'coefficient': model_final.coef_[0],\n",
    "    'abs_coefficient': np.abs(model_final.coef_[0])\n",
    "}).sort_values('abs_coefficient', ascending=False)\n",
    "\n",
    "print(\"Top 20 Most Important Features:\")\n",
    "print(\"=\" * 60)\n",
    "for i, row in importance.head(20).iterrows():\n",
    "    direction = '+' if row['coefficient'] > 0 else '-'\n",
    "    # Highlight Glassnode features\n",
    "    marker = '*' if row['feature'] in GLASSNODE_FEATURES else ' '\n",
    "    print(f\"{marker} {row['feature']:45s} {direction} ({row['abs_coefficient']:.4f})\")\n",
    "\n",
    "print(\"\\n* = Glassnode feature\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot feature importance\n",
    "fig, ax = plt.subplots(figsize=(10, 10))\n",
    "\n",
    "top_features = importance.head(20)\n",
    "colors = ['green' if c > 0 else 'red' for c in top_features['coefficient']]\n",
    "\n",
    "# Highlight Glassnode features with different edge\n",
    "edge_colors = ['blue' if f in GLASSNODE_FEATURES else 'black' for f in top_features['feature']]\n",
    "\n",
    "bars = ax.barh(range(len(top_features)), top_features['coefficient'], color=colors, alpha=0.7, edgecolor=edge_colors, linewidth=2)\n",
    "ax.set_yticks(range(len(top_features)))\n",
    "ax.set_yticklabels(top_features['feature'])\n",
    "ax.set_xlabel('Coefficient')\n",
    "ax.set_title('Top 20 Feature Importance\\n(Blue border = Glassnode feature)')\n",
    "ax.axvline(x=0, color='black', linewidth=0.5)\n",
    "ax.invert_yaxis()\n",
    "ax.grid(True, alpha=0.3, axis='x')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 10. Comparison: Phase 1 vs Phase 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Summary comparison\n",
    "print(\"PHASE COMPARISON\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"\\n{'Metric':<25} {'Phase 1':>12} {'Phase 2':>12} {'Change':>12}\")\n",
    "print(\"-\" * 60)\n",
    "\n",
    "# Phase 1 results (from previous notebook - approximate values)\n",
    "phase1_best_sharpe = 3.93  # at τ=0.6\n",
    "phase1_best_return = 11.71\n",
    "\n",
    "# Phase 2 best results\n",
    "best_idx = threshold_df['sharpe'].idxmax()\n",
    "phase2_best_sharpe = threshold_df.loc[best_idx, 'sharpe']\n",
    "phase2_best_return = threshold_df.loc[best_idx, 'total_return'] * 100\n",
    "phase2_best_threshold = threshold_df.loc[best_idx, 'threshold']\n",
    "\n",
    "print(f\"{'Features':<25} {'4':>12} {len(X.columns):>12} {f'+{len(X.columns)-4}':>12}\")\n",
    "print(f\"{'Best Sharpe':<25} {phase1_best_sharpe:>12.3f} {phase2_best_sharpe:>12.3f} {phase2_best_sharpe - phase1_best_sharpe:>+12.3f}\")\n",
    "print(f\"{'Best Threshold':<25} {'0.6':>12} {phase2_best_threshold:>12.1f} {'-':>12}\")\n",
    "print(f\"{'Return at Best':<25} {phase1_best_return:>11.2f}% {phase2_best_return:>11.2f}% {phase2_best_return - phase1_best_return:>+11.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 11. Summary\n",
    "\n",
    "### What we added:\n",
    "1. **Glassnode on-chain metrics** (10 features)\n",
    "   - MVRV Z-Score, Fear & Greed, SOPR, Puell Multiple, etc.\n",
    "2. **Technical indicators** (15+ features)\n",
    "   - RSI, Bollinger Bands, MACD\n",
    "\n",
    "### Key findings:\n",
    "- [ ] Fill in after running\n",
    "\n",
    "### Next steps:\n",
    "- Phase 3: Optimize label horizon and costs\n",
    "- Phase 4: Try better models (XGBoost)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save results\n",
    "output_dir = Path.cwd().parent / 'data' / 'processed'\n",
    "output_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "predictions_df.to_csv(output_dir / 'phase2_predictions.csv')\n",
    "print(f\"Saved predictions to {output_dir / 'phase2_predictions.csv'}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
